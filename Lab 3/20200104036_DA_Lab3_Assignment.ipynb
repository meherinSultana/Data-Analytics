{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptvFd2IXBTY5",
        "outputId": "144af63f-3643-4c68-9402-ad20f43ec6fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/spark-3.0.3-bin-hadoop3.2/python/pyspark/sql/session.py:381: UserWarning: inferring schema from dict is deprecated,please use pyspark.sql.Row instead\n",
            "  warnings.warn(\"inferring schema from dict is deprecated,\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+\n",
            "|hello|\n",
            "+-----+\n",
            "|world|\n",
            "|world|\n",
            "|world|\n",
            "+-----+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "# Install Spark (change the version number if needed)\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.0.3/spark-3.0.3-bin-hadoop3.2.tgz\n",
        "\n",
        "# Unzip the Spark file to the current folder\n",
        "!tar xf spark-3.0.3-bin-hadoop3.2.tgz\n",
        "\n",
        "# Install findspark\n",
        "!pip install -q findspark\n",
        "\n",
        "# Set environment variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.3-bin-hadoop3.2\"\n",
        "\n",
        "# Start a SparkSession\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "# Import SparkSession\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "\n",
        "# Test Spark\n",
        "df = spark.createDataFrame([{\"hello\": \"world\"} for x in range(1000)])\n",
        "df.show(3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.types import IntegerType, DoubleType\n",
        "from pyspark.ml.feature import VectorAssembler, StandardScaler"
      ],
      "metadata": {
        "id": "UntL9AznOdhS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.csv('/content/customer_shopping_data.csv',header=True,escape=\"\\\"\")\n",
        "\n",
        "df_sample = df.sample(withReplacement=False, fraction=0.1, seed=42).limit(10000)\n",
        "\n",
        "df_sample.printSchema()\n",
        "df_sample.show(5)\n",
        "\n",
        "df_sample = df_sample.withColumn(\"quantity\", col(\"quantity\").cast(DoubleType()))\n",
        "df_sample = df_sample.withColumn(\"price\", col(\"price\").cast(DoubleType()))\n",
        "df_sample = df_sample.withColumn(\"age\", col(\"age\").cast(IntegerType()))\n",
        "\n",
        "df_sample = df_sample.dropna()\n",
        "\n",
        "feature_columns = [\"quantity\", \"price\", \"age\"]\n",
        "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
        "assembled_df = assembler.transform(df_sample)\n",
        "\n",
        "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withStd=True, withMean=False)\n",
        "scaler_model = scaler.fit(assembled_df)\n",
        "scaled_df = scaler_model.transform(assembled_df)\n",
        "\n",
        "scaled_df.select(\"features\", \"scaledFeatures\").show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_fs8bzXLEZe",
        "outputId": "10327660-19b7-4b5c-f09d-f36cdfbb91ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- invoice_no: string (nullable = true)\n",
            " |-- customer_id: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- age: string (nullable = true)\n",
            " |-- category: string (nullable = true)\n",
            " |-- quantity: string (nullable = true)\n",
            " |-- price: string (nullable = true)\n",
            " |-- payment_method: string (nullable = true)\n",
            " |-- invoice_date: string (nullable = true)\n",
            " |-- shopping_mall: string (nullable = true)\n",
            "\n",
            "+----------+-----------+------+---+---------------+--------+------+--------------+------------+-----------------+\n",
            "|invoice_no|customer_id|gender|age|       category|quantity| price|payment_method|invoice_date|    shopping_mall|\n",
            "+----------+-----------+------+---+---------------+--------+------+--------------+------------+-----------------+\n",
            "|   I293112|    C176086|Female| 32|       Clothing|       2|600.16|   Credit Card|  13/01/2021| Mall of Istanbul|\n",
            "|   I294687|    C300786|  Male| 65|          Books|       2|  30.3|    Debit Card|  16/01/2021|        Metrocity|\n",
            "|   I993048|    C218149|Female| 46|       Clothing|       2|600.16|          Cash|  26/07/2021|     Metropol AVM|\n",
            "|   I246562|    C227070|Female| 61|      Cosmetics|       5| 203.3|          Cash|  23/08/2021|Emaar Square Mall|\n",
            "|   I263803|    C112279|Female| 67|Food & Beverage|       3| 15.69|          Cash|  23/06/2021|           Kanyon|\n",
            "+----------+-----------+------+---+---------------+--------+------+--------------+------------+-----------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+-----------------+--------------------+\n",
            "|         features|      scaledFeatures|\n",
            "+-----------------+--------------------+\n",
            "|[2.0,600.16,32.0]|[1.40851466705288...|\n",
            "|  [2.0,30.3,65.0]|[1.40851466705288...|\n",
            "|[2.0,600.16,46.0]|[1.40851466705288...|\n",
            "| [5.0,203.3,61.0]|[3.52128666763220...|\n",
            "| [3.0,15.69,67.0]|[2.11277200057932...|\n",
            "+-----------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.clustering import KMeans, GaussianMixture\n",
        "from pyspark.ml.evaluation import ClusteringEvaluator"
      ],
      "metadata": {
        "id": "7Ky4rL8XP_DC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# K-means Clustering\n",
        "kmeans = KMeans(featuresCol=\"scaledFeatures\", k=5, seed=13)\n",
        "kmeans_model = kmeans.fit(scaled_df)\n",
        "kmeans_predictions = kmeans_model.transform(scaled_df)\n",
        "\n",
        "# Gaussian Mixture Models (GMM) Clustering\n",
        "gmm = GaussianMixture(featuresCol=\"scaledFeatures\", k=5, seed=13)\n",
        "gmm_model = gmm.fit(scaled_df)\n",
        "gmm_predictions = gmm_model.transform(scaled_df)\n",
        "\n",
        "# Evaluate Clustering Performance\n",
        "evaluator = ClusteringEvaluator(featuresCol=\"scaledFeatures\")\n",
        "\n",
        "# Silhouette Score\n",
        "kmeans_silhouette = evaluator.evaluate(kmeans_predictions)\n",
        "gmm_silhouette = evaluator.evaluate(gmm_predictions)\n",
        "print(f\"K-means Silhouette Score = {kmeans_silhouette}\")\n",
        "print(f\"GMM Silhouette Score = {gmm_silhouette}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8_elUoGOlvw",
        "outputId": "a5a68241-8b0d-4762-bd43-2af6ef0bf52d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "K-means Silhouette Score = 0.484626984314677\n",
            "GMM Silhouette Score = 0.1719341034165961\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Davies-Bouldin Index\n",
        "def davies_bouldin_index(predictions, features_col, prediction_col=\"prediction\"):\n",
        "    from pyspark.ml.linalg import Vectors\n",
        "    from sklearn.metrics import davies_bouldin_score\n",
        "    pdf = predictions.select(features_col, prediction_col).toPandas()\n",
        "    X = list(pdf[features_col].apply(lambda x: Vectors.dense(x).toArray()))\n",
        "    labels = pdf[prediction_col]\n",
        "    return davies_bouldin_score(X, labels)\n",
        "\n",
        "kmeans_dbi = davies_bouldin_index(kmeans_predictions, \"scaledFeatures\")\n",
        "gmm_dbi = davies_bouldin_index(gmm_predictions, \"scaledFeatures\")\n",
        "print(f\"K-means Davies-Bouldin Index = {kmeans_dbi}\")\n",
        "print(f\"GMM Davies-Bouldin Index = {gmm_dbi}\")\n",
        "\n",
        "# Calinski-Harabasz Index\n",
        "def calinski_harabasz_index(predictions, features_col, prediction_col=\"prediction\"):\n",
        "    from pyspark.ml.linalg import Vectors\n",
        "    from sklearn.metrics import calinski_harabasz_score\n",
        "    pdf = predictions.select(features_col, prediction_col).toPandas()\n",
        "    X = list(pdf[features_col].apply(lambda x: Vectors.dense(x).toArray()))\n",
        "    labels = pdf[prediction_col]\n",
        "    return calinski_harabasz_score(X, labels)\n",
        "\n",
        "kmeans_chi = calinski_harabasz_index(kmeans_predictions, \"scaledFeatures\")\n",
        "gmm_chi = calinski_harabasz_index(gmm_predictions, \"scaledFeatures\")\n",
        "print(f\"K-means Calinski-Harabasz Index = {kmeans_chi}\")\n",
        "print(f\"GMM Calinski-Harabasz Index = {gmm_chi}\")\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WrLdEWQQDwo",
        "outputId": "ee0d3f01-1c32-44fb-ec7a-9c0037605a01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "K-means Davies-Bouldin Index = 0.9934456903165045\n",
            "GMM Davies-Bouldin Index = 1.8933869200209297\n",
            "K-means Calinski-Harabasz Index = 5447.753674244457\n",
            "GMM Calinski-Harabasz Index = 2048.7572789884725\n"
          ]
        }
      ]
    }
  ]
}